\chapter{Microservizio EventExport}
\label{cap:MicroservizioEventExport}

\section{Panoramica del microservizio}
\label{sec:ScopoDelMicroservizio}
Lo scopo del microservizio \textit{EventExport} è quello di elaborare dei \textbf{Business Event}, che arrivano dal microservizio \textit{EventEngine} 
(sezione \ref{subsubsec:event_engine}) e di comunicarli al cliente.\\\\
\textbf{EventExport} è configurabile in maniera differente per ogni cliente, di modo che ad un cliente vengano inviati solo alcuni tipi di eventi.
Ad esempio, un cliente potrebbe essere interessato solo agli eventi di creazione di un ordine, mentre un altro potrebbe voler ricevere tutti gli aggiornamenti
riguardo alla posizione dell'ordine da lui effettuato.
Questa configurazione è gestita tramite una tabella \textit{TradingPartnerEventLookup} che contiene le regole di filtro per ogni cliente.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/EventExport/EventExport_architecture.png}
    \caption{architettura con il microservizio \textit{EventExport}}
    \label{fig:EventExport_architecture}
\end{figure}\\
Il microservizio compie le seguenti operazioni:
\begin{enumerate}
    \item Consumare il \textit{Signaling Topic} (topic Kafka) applicando un filtro basato su una tabella di configurazione.
    \item Controllare se l'evento deve essere inviato. Non è necessario inviare eventi già inviati a meno di modifiche su campi significati 
    (anche questi definiti in base alla configurazione).
    \item Creare un file XML con i dati richiesti dal cliente. I dati sono definiti in base a dei template che vengono compilati in base alla configurazione.
    \item Inviare il file XML ad un altro servizio che si occuperà di inviare al cliente una mail.
    \item Loggare l'evento, compreso di XML, in una tabella (\textit{TransportOrdersExportEvents}) per tenere traccia degli eventi inviati.
\end{enumerate}

\section{Lettura del Signaling Topic}
\label{sec:LetturaDelSignalingTopic}
La lettura del \textit{Signaling Topic} avviene tramite un \textit{Consumer} Kafka.
\texttt{Flink} mette a disposizione un \textit{Kafka Consumer} che permette agevolmente di leggere i messaggi da un topic Kafka: \texttt{org.apache.flink.connector.kafka.source.KafkaSource}
Il \textit{Broker} Kafka su cui vive il \textit{Signaling Topic} è configurato per usare un sistema di autenticazione basato su \textit{SASL/SSL},
inoltre gli eventi sul topic sono serializzati tramite \textit{Apache Avro} (sezione \ref{subsec:avro_overview}) quindi è necessario configurare il \textit{Consumer Kafka} di conseguenza.
\begin{code}
    \inputminted[linenos]{java}{listings/EventsExport/KafkaSources.java}
    \caption{Configurazione del Kafka Consumer}
    \label{lst:KafkaSources}
\end{code}
Come mostrato nel codice \ref{lst:KafkaSources}, il \textit{Consumer Kafka} è configurato per leggere i messaggi dal topic \textit{Signaling Topic} 
ed il deserializzatore utilizzato è\\ \texttt{org.apache.flink.api.common.serialization.DeserializationSchema},
una classe messa a disposizione da Flink per deserializzare i messaggi \textit{Avro}.
Le righe 15,18,19 e 26 sono tipiche di un \textit{Consumer Kafka} scritto con \textit{Flink}, è presente infatti la creazione del \textit{builder} (riga 15),
la definizione del \textit{topic} da cui leggere (riga 18), l'offset da cui iniziare a leggere (riga 19) e  la creazione del \textit{Consumer} (riga 26).
Invece nelle righe 16 e 20-21 si possono leggere rispettivamente la configurazione del \textit{Kafka Consumer} e la configurazione del deserializzatore \textit{Avro}.\\\\
Nella chiamata alla funzione \texttt{forSpecific} a riga 21 i parametri sono la classe in cui verrà deserializzato il messaggio, l'url dello \textit{Schema Registry}
e le properties contenenti credenziali per accedere allo \textit{Schema Registry} \footnote{Lo \textit{Schema Registry} può essere visto come un web server che fornisce
gli \textit{Schema} corrispondenti ai messaggi \texttt{Avro} che vengono ricevuti. Una descrizione più dettagliata si può trovare in sezione \ref{subsec:avro_overview}}. 
La configurazione del \textit{Kafka Consumer} e del deserializzatore \textit{Avro} è definita in un file \textit{.properties} e principalmente contiene
l'url dell'endpoint, le credenziali e diversi parametri per la connessione.
In un caso (\textit{Kafka}) è riferita al \textit{Broker Kafka} e nell'altro allo \textit{Schema Registry} (\textit{Avro}).
\subsection{Struttura del dato}
\label{subsec:StrutturaDelDato}
Il dato letto dal \textit{Signaling Topic} è un \textit{Business Event} che contiene una serie di campi. 
La maggior parte di questi non è rilevante per il microservizio \textit{EventExport}, ma alcuni sono fondamentali per la comprensione del suo funzionamento.
\begin{itemize}
    \item DomainKey: chiave di dominio, identifica un dato specificando il sistema che l'ha generato, il cliente a cui fa riferimento e l'identificativo del dato. non è univoca.
    \item BusinessObjectType: tipo di oggetto, può essere 1 - spedizione, 2 - ritiro, 3 - viaggio, 4 - disposizione o 5 - ordine.
    \item TradingPartner: cliente a cui fa riferimento il dato.
    \item Action: azione svolta sul dato, può essere Create, Update, Delete.
    \item SignalingEvent: Lista di eventi relativi a quel dato.
\end{itemize}
Ogni \textit{Business Event} contiene una lista di \textit{Signaling Event}, ognuno dei quali rappresenta un cambiamento di stato del dato. I campi più rilevanti di un \textit{Signaling Event} sono:
\begin{itemize}
    \item EventCode: Codice ERP dell'evento, identifica una categoria di eventi (STC, DCI, STD) \todo{esattamente cosa vogliano dire questi codici non lo so, informarsi} .
    \item OperationType: Tipo di operazione, può essere c - Creazione, u - Modifica,udd - Cancellazione.
    \item EventUser: utente che ha generato l'evento. 
\end{itemize}

\section{Lettura della tabella di configurazione}
\label{sec:LetturaDellaTabellaDiConfigurazione}
La tabella di configurazione è una tabella \textit{SQL} chiamata \textit{TradingPartnerEventLookup} e contiene le regole di filtro per ogni cliente.
La connessione al database è gestita tramite \textit{JPA} e la libreria \textit{Hibernate} (sezione \ref{subsec:hibernate_overview}) con un sistema di caching
per evitare richieste ripetute al database. La cache viene aggiornata ogni ora.
\begin{table}[!ht]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
        Id & TradingPartner & OT & ERP & ID & Depot & Active & QueryCondition \\ \hline
        1 & TEST\_1 & 5 & 08 & null & null & Y & null \\ \hline
        2 & TEST\_1 & 5 & 20 & null & null & Y & null \\ \hline
        3 & TEST\_1 & 5 & 21 & null & null & Y & null \\ \hline
        4 & TEST\_1 & 5 & GEO & null & null & Y & null \\ \hline
        5 & TEST\_2 & 1 & COR & null & null & Y & null \\ \hline
        6 & TEST\_2 & 1 & GEN & null & null & Y & null \\ \hline
        7 & TEST\_3 & 5 & 10 & null & null & Y & null \\ \hline
        8 & TEST\_4 & 1 & COR & null & null & Y & null \\ \hline
        9 & TEST\_4 & 1 & VIC & null & null & Y & null \\ \hline
        10 & TEST\_4 & 1 & VSC & null & null & Y & null \\ \hline
        11 & TEST\_5 & 1 & CMG & null & null & Y & null \\ \hline
        12 & TEST\_5 & 1 & CMO & null & null & Y & null \\ \hline
        13 & TEST\_5 & 1 & CMR & null & null & Y & null \\ \hline
        14 & TEST\_5 & 1 & COR & null & null & Y & null \\ \hline
        15 & TEST\_5 & 1 & ERR & null & null & Y & null \\ \hline
        16 & TEST\_5 & 1 & GIA & null & null & Y & null \\ \hline
        17 & TEST\_5 & 1 & OAC & null & null & Y & null \\ \hline
        18 & TEST\_5 & 1 & STC & null & null & Y & null \\ \hline
    \end{tabular}%
    }
    \caption{Tabella TradingPartnerEventLookup di esempio}
    \label{tab:TradingPartnerEventLookup}
\end{table}
La tabella \ref{tab:TradingPartnerEventLookup} è un esempio di come potrebbe essere configurato il microservizio per inviare eventi a diversi clienti, I campi più rilevanti sono:
\begin{itemize}
    \item \textbf{TradingPartner}: il nome del cliente, in questo caso sono tutti clienti fittizi di test.
    \item \textbf{OT}: \textit{BusinessObjectType} il tipo di evento (1 Spedizione – 2 Ritiro – 5 Ordini).
    \item \textbf{ERP}: Lo specifico evento, ad esempio \textit{08} è l'evento di creazione di un ordine, \textit{GEO} è l'evento di modifica della posizione di un ordine.
    \item \textbf{Active}: se il filtro è attivo.
\end{itemize}
\texttt{Flink} è basato sulla computazione distribuita di stream di dati (vedasi sezione \ref{sec:flink_overview}),
ciò va tenuto in conto durante la creazione della cache per la tabella di configurazione. 
Infatti implementando una semplice cache locale (attraverso una \texttt{HashMap} ad esempio), si potrebbero avere problemi di consistenza dei dati e di sincronizzazione tra i vari nodi.
Inoltre tale implementazione non sfrutta le potenzialità di \textit{Flink} e la sua gestione automatica della scalabilità.
Per mostrare le problematiche di una cache implementata in maniera non corretta, si consideri la seguente situazione d' esempio:\\
\textit{All'interno di un \textbf{job}  viene creato un metodo che legge la tabella di configurazione e la memorizza in una \texttt{HashMap}, ogni 10 minuti la cache viene aggiornata.}\\
Automaticamente il \textit{job} viene scalato su più nodi da \textit{Flink}, ogni nodo avrà la sua copia della cache e tale cache verrà aggiornata in maniera indipendente.
Questo comporta prima di tutto un problema di consistenza dei dati, in quanto un nodo potrebbe avere una copia della cache diversa 
qualora fosse impegnato in una computazione al momento dell'aggiornamento.
Ma soprattutto ogni nodo effettua una query al database in fase di aggiornamento, causando una serie di richieste tutti identiche e quindi ridondanti.
Se ad esempio il \textit{job} viene scalato su 5 nodi, ci saranno 5 richieste al database ad ogni aggiornamento, quando invece ne basterebbe una sola.\\\\
Il modo corretto di gestire questa cache è di utilizzare un datastream che legga la tabella di configurazione e la mantenga aggiornata, in collaborazione con il pattern 
\textbf{Broadcast State Pattern} per la distribuzione della cache tra i vari nodi.

\subsection{Rules Based Stream Processing}
\label{subsec:RulesBasedStreamProcessing}
Il \textit{Broadcast State Pattern} è un pattern di \textit{Flink} che permette di distribuire uno stato tra tutti i nodi di un job.
Questo stato è distribuito in maniera efficiente e scalabile, inoltre è possibile aggiornarlo in maniera asincrona.
Il \textit{Broadcast State Pattern} è fondamentale per la creazione di un sistema di \textit{Rules Based Stream Processing}, 
cioè un sistema che applica delle regole ad un flusso di dati.\\
Ci sono quindi due flussi di dati, uno contenente le regole e l'altro i dati a cui applicare le regole.
Ogni nodo mantiene un \textit{Broadcast State}, cioè una \texttt{Map} che viene aggiornata all'arrivo di nuove regole.
Quando una nuova regola arriva dalla sorgente del flusso di regole, questa viene distribuita a tutti i nodi che andranno ad aggiornare il loro \textit{Broadcast State}
con la nuova regola, secondo quanto definito nel codice. In questo modo ogni nodo ha sempre una copia aggiornata delle regole da poter confrontare con i dati in arrivo sul flusso principale.
Quando invece un dato arriva sul flusso principale questi viene direttamente analizzato dal nodo che, eventualmente avvalendosi del \textit{Broadcast State}, lo elabora 
secondo quanto definito dalla logica implementativa.\\\\
Ad esempio (\todo{esempio preso dala documentazione di FLink, citarlo}), se una compagnia volesse analizzare le abitudini di acquisto su un e-commerce dei loro clienti 
potrebbe avvalersi di un \textit{Rules Based Stream Processing}. Quindi si avrebbero due stream, uno contenente tutte le operazioni svolte da un utente,
l'altro contenente il pattern che vogliamo analizzare, come mostrato in figura \ref{fig:broadcastState1}.
\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.7\textwidth]{images/EventExport/broadcastState1.png}
    \caption{Stream di dati e stream di regole}
    \label{fig:broadcastState1}
\end{figure}
Appena arriva una nuova regola, questa viene distribuita a tutti i nodi, che aggiorneranno i loro \textit{Broadcast State}.
Nel caso in figura \ref{fig:broadcastState2} ad esempio, il pattern appena giunto è l'operazione di ingresso al sito seguita da un'uscita, senza acquisti.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/EventExport/broadcastState2.png}
    \caption{Aggiornamento del \textit{Broadcast State}}
    \label{fig:broadcastState2}
\end{figure}
Successivamente, quando arriva un nuovo dato, questo viene analizzato dal nodo che, avvalendosi del \textit{Broadcast State}, lo elabora secondo quanto definito dalla regola.
Ad esempio nella sezione sinistra della figura \ref{fig:broadcastState3/4}, si può notare che viene ricevuto il seguente dato: \textit{il cliente 1001 entra nel sito}.
Tale evento viene correttamente inviato al primo nodo (il nodo più in alto in figura), possiamo infatti supporre che l'id del cliente sia la chiave di partizionamento.
Nel frattempo gli altri due clienti effettuano altre operazioni che vengono inviate ai relativi nodi.
Successivamente arriva un altro dato: \textit{"il cliente 1001 esce dal sito"} (parte destra della figura), che viene di nuovo inviato al primo nodo.
Questo nodo, avendo memoria della regola impostata nel \textit{Broadcast State}, riconosce il pattern e lo invia al sistema di analisi.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{images/EventExport/broadcastState3.png}
    \includegraphics[width=0.49\textwidth]{images/EventExport/broadcastState4.png}
    \caption{Esempio di pattern riconosciuto}
    \label{fig:broadcastState3/4}
\end{figure}

\subsection{Implementazione del Rules Based Stream Processing}
\label{subsec:ImplementazioneDelRulesBasedStreamProcessing}
Come descritto in precedenza (sezione \ref{subsec:RulesBasedStreamProcessing}), il Rules Based Stream Processing è un sistema che applica delle regole ad un flusso di dati.
In questo caso le regole sono le letture della tabella \textit{TradingPartnerEventLookup} e il flusso di dati è quello dei \textit{Business Event} provenienti dal \textit{Signaling Topic}
\begin{code}
    \inputminted[linenos,fontsize=\footnotesize]{java}{listings/EventsExport/TradingPartnerBroadcast.java}
    \caption{creazione del Broadcast Stream}
    \label{lst:EventExportRulesBasedStreamProcessing}
\end{code}
La prima operazione, mostrata nel codice \ref{lst:EventExportRulesBasedStreamProcessing}, è la creazione del \texttt{Broadcast Stream} partendo dallo stream \texttt{TradingPartnerEventLookupStream}.
Tale stream è un \textit{DataStream} che legge la tabella \textit{TradingPartnerEventLookup} ogni ora e ne mantiene una copia aggiornata. 
Ogni evento presente in tale stream corrisponde ad una intera copia della tabella, infatti il tipo di dato che lo compone è \texttt{List<TradingPartnerEventLookup>}, 
cioè una lista di regole (ogni regola corrisponde ad una riga della tabella \textit{TradingPartnerEventLookup}).\\\\
Ottenuto il \texttt{Broadcast Stream} si può procedere con il collegamento al flusso principale, cioè quello dei \textit{Business Event},
di cui è già stata trattata la creazione nella sezione \ref{sec:LetturaDelSignalingTopic}. 
\begin{code}
    \inputminted[linenos,fontsize=\footnotesize]{java}{listings/EventsExport/StreamConnection.java}
    \caption{collegamento del \textit{Broadcast Stream} allo stream di dati}
    \label{lst:StreamsConnection}
\end{code}
Come prima operazione c'è una fase di filtro dei record \texttt{SignalingTopicRecord} che non hanno \texttt{SignalingEvent} (cioè non hanno ricevuto aggiornamenti, 
oppure sono appena stati creati e non ancora popolati), questo perché non è necessario processare eventi che non hanno dati.
Poi si procede con una operazione di mappatura, per comodità di elaborazione, in cui si genera un \texttt{Tuple2} contenente il \texttt{SignalingTopicRecord}
ed un \texttt{SignalingEvent}, questo per ognuno dei \texttt{SignalingEvent} ad esso associati.
Dopo un ultima fase di filtro, atta ad eliminare le tuple che indicano operazioni di cancellazione (riga 12), si procede con la connessione al \textit{Broadcast Stream}
tramite il metodo \texttt{connect} (riga 13).
Infine si applica il \textit{Broadcast State pattern}, operazione effettuata dal metodo \texttt{process} (riga 14).\\\\
La classe \texttt{TradingPartnerRuleEvaluator} è una classe che estende la classe \texttt{BroadcastProcessFunction} ed è la classe che si occupa di applicare il \textit{Broadcast State pattern}.
La sua implementazione è mostrata nel codice \ref{lst:TradingPartnerRuleEvaluator}.
\begin{code}
    \inputminted[linenos,fontsize=\footnotesize]{java}{listings/EventsExport/TradingPartnerRuleEvaluator.java}
    \caption{Implementazione della classe \texttt{TradingPartnerRuleEvaluator}}
    \label{lst:TradingPartnerRuleEvaluator}
\end{code}
Le due funzioni, \texttt{processElement} e \texttt{processBroadcastElement}, sono rispettivamente la funzione che processa i dati del flusso principale e la funzione che processa i dati del \textit{Broadcast Stream}.
\begin{itemize}
    \item \texttt{processBroadcastElement} (riga 38) è la funzione che processa i dati del \textit{Broadcast Stream}, cioè le regole.
    Quando arriva un nuovo \textit{Broadcast Element} (cioè una nuova lista di regole) questa funzione aggiorna il \textit{Broadcast State} con le nuove regole.
    Per prima cosa recupera il \textit{Broadcast State} tramite il suo \texttt{Descriptor} (riga 42) e poi scorre la lista di regole generando una \texttt{Map} (riga 44).
    Questa \texttt{Map} è la rappresentazione delle regole in un formato più efficiente, in cui il valore è la regola stessa e la chiave è una stringa che identifica univocamente la regola
    (\textit{TradingPartner + BusinessObjectType + ERPCode}).
    \item \texttt{processElement} (riga 17) è la funzione che processa i dati del flusso principale, cioè i \textit{Business Event}.
    Quando un nuovo elemento arriva questa funzione per prima cosa ricalcola la chiave con cui andare a cercare le regole nel \textit{Broadcast State} (riga 20).
    Poi controlla se la chiave è presente nel \textit{Broadcast State} (riga 24), se non è presente viene segnalato l'errore tramite log4j \todo{parlare di cos'è log4j}, altrimenti si procede con l'elaborazione.
    Nell'elaborazione avviene un controllo riguardo lo stato della regola (riga 27), se la regola è attiva viene restituita una tripla (Tuple3) così composta:
    \begin{itemize}
        \item \texttt{SignalingTopicRecord}: Record ottenuto dal \textit{Topic}.
        \item \texttt{SignalingEvent\_record}: Evento singolo ottenuto dal \texttt{Signaling\-Topic\-Record}.
        \item \texttt{TradingPartnerEventLookup}: la regola relativa all'evento.
    \end{itemize}
\end{itemize}

\section{Lettura della tabella di Determinazione}
\label{sec:LetturaDellaTabellaDiDeterminazione}
Una volta associato un evento ad una regola, si procede con l'analisi dell'evento. La tabella \textit{TradingPartnerEventLookup} infatti definisce solo se il cliente vuole aggiornamenti sul dato,
ma non definisce per quali eventi è necessario un aggiornamento e per quali no. 
Il dato va quindi analizzato per capire se è necessario inviarlo al cliente e, in caso affermativo, quali dati devono essere inseriti nel messaggio.
Le specifiche di questa operazione sono definite in un'altra tabella, chiamata \textit{LogicDeterminationEvent}.
Nella tabella sono specificate le regole di determinazione per ogni evento, ad esempio se l'aggiunta di note, la creazione o la modifica 
comportano l'invio di un messaggio di avviso al cliente.
A livello implementativo questa tabella è gestita in maniera simile alla tabella \textit{TradingPartnerEventLookup}, tramite il \textit{Broadcast State pattern}.

\section{Conversione del dato in XML}
\label{sec:ConversioneDelDatoInXML}
Una volta ottenute tutte le regole necessarie per l'elaborazione, si procede con la creazione del messaggio da inviare al cliente.
Per fare ciò, rispettando la logica a \textit{Datastream} di \textit{Flink}, si procede con la creazione di un nuovo oggetto che rappresenta il messaggio da inviare.
Tale oggetto viene arricchito in passi successivi tramite operazioni di \textit{map} e \textit{filter}.
L'arricchimento avviene tramite l'aggiunta di campi al messaggio, a seconda delle regole di determinazione, prendendo eventuali informazioni aggiuntive dal database.
In questo caso non c'è la necessità di utilizzare il \textit{Broadcast State pattern}, in quanto le richieste sono sporadiche e molto varie,
inserire un meccanismo di \textit{caching} non porterebbe benefici.
Quindi si procede con una semplice query al database per ottenere le informazioni necessarie, sfruttando il \textit{Repository Pattern}.
\begin{code}
    \inputminted[linenos,fontsize=\footnotesize]{java}{listings/EventsExport/RepositoryPattern.java}
    \caption{Esempio di \textit{Repository Pattern}}
    \label{lst:RepositoryPattern}
\end{code}
Come mostrato nel codice d'esempio \ref{lst:RepositoryPattern}, il \textit{Repository Pattern} viene implementato tramite \textit{Hibernate} e \textit{JPA} (sezione \ref{subsec:hibernate_overview}).
Le query sono scritte in \textit{HQL} e vengono eseguite tramite il metodo \texttt{findQuery} della classe \texttt{HibernateUtils}.
Allo stesso modo, le scritture sul database vengono effettuate tramite il metodo \texttt{save} della classe \texttt{HibernateUtils}.\\\\
Le operazioni di arricchimento dell'oggetto \texttt{ExportEventToSend} non sono mostrate in dettaglio, in quanto sono logiche di business non rilevanti nell'ambito di questa tesi.
Comunque in seguito a queste operazioni si ottiene un oggetto \texttt{ExportEventToSend} completo ed anche la sua rappresentazione in formato \textit{XML}.
L'oggetto viene quindi salvato, sempre tramite \textit{Hibernate}, in una tabella di log (\textit{TransportOrdersExportEvents}) per scopi di logging.
Il record contiene tutti i dati dell'oggetto, compreso di regole di determinazione, regole di configurazione ed il messaggio in formato \textit{XML}.
Il messaggio in formato \textit{XML} viene poi inviato al cliente.

\section{Invio del messaggio}
\label{sec:InvioDelMessaggio}
L'invio del messaggio avviene tramite un servizio esterno, chiamato \textit{IBM Sterling}, che si occupa di inviare mail ai clienti.
Si comunica con questo servizio tramite un servizio \textit{FTP}, in cui si carica il file \textit{XML},
\textit{Sterling} periodicamente controlla la presenza di nuovi file e li invia ai clienti.
Dal punto di vista implementativo ciò avviene tramite ciò che in \textit{Flink} viene definito \textbf{User-defined Sinks},
cioè una classe che estende la classe \texttt{SinkFunction} e che funge da endpoint per un \textit{datastream} (come discusso nella sezione \ref{subsec:datastream_api}).
Ciò viene fatto tramite una semplice chiamata al metodo \texttt{addSink} del \textit{datastream} che contiene il messaggi da inviare.
\mintinline{java}|eventToSendDataStream.addSink(new sendXMLSink())|
\begin{absolutelynopagebreak}
\begin{code}
    \inputminted[linenos,fontsize=\footnotesize]{java}{listings/EventsExport/sendXMLSink.java}
    \caption{User-defined Sink per l'invio del messaggio}
    \label{lst:sendXMLSink}
\end{code}
\end{absolutelynopagebreak}
La classe \texttt{sendXMLSink} è una classe che estende la classe \texttt{SinkFunction} e che implementa il metodo \texttt{invoke}.
Come mostrato nel codice \ref{lst:sendXMLSink} nel \textit{Constructor} della classe viene inizializzato il servizio \texttt{MailboxService},
che si occupa effettivamente di inviare il messaggio.
Il metodo \texttt{invoke} invece è il metodo che viene chiamato per ogni record del \textit{datastream},
cioè per ogni \texttt{ExportEvent} pronto ad essere spedito.
In questo metodo si richiede quindi al servizio \texttt{MailboxService} di creare il documento sul server \textit{FTP} e poi,
grazie ad un \textit{id} ritornato dal servizio, di inserire nella coda di invio il documento.\\\\
Le operazioni specifiche di invio sono nascoste all'interno della classe \texttt{MailboxService}.
Nel metodo \texttt{CreateDocument} viene creata una richiesta \textit{HTTP} ad un URL specifico per la creazione del documento (\textit{http://ServerDomain/svc/document}).,
nel cui corpo è presente il messaggio \textit{XML} codificato in base 64.
La richiesta viene effettuata da un \textit{HTTP client} che, se il documento è stato caricato correttamente, restituisce il path del documento creato.
Da questo path si ottiene l'\textit{id} del documento, che viene utilizzato per inserire il documento nella coda di invio.
Infatti nel metodo \texttt{addMessageToMailBox} si indica a \textit{Sterling} di inserire il documento nella coda di invio tramite una richiesta \textit{HTTP}
ad un altro URL (\textit{http://ServerDomain/svc/mailbox}).
Tale richiesta contiene nel corpo l'\textit{id} del documento ed il nome del file, che diventerà l'oggetto dell' email.
Il destinatario viene automaticamente ricavato da \textit{Sterling} tramite quanto scritto nell'XML.
\todo{chiedere alla Petrone se è il caso di approfondire o no, di base si tratta di semplici chiamate effettuate tramite HTTP, tutta business logic}

